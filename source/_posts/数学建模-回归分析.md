---
title: 数学建模-回归分析
date: 2021-05-14 20:01:12
mathjax: true
tag: [数学建模]
---



# 回归分析

回归分析的使命：

1. 判断哪些 X 变量是同 Y 真的相关
2. 判断相关性是正的还是负的
3. 估计回归系数权重（还要标准化去量纲之后）

回归分析的分类：

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210422162048564.png)

回归的方法和评价跟 **拟合** 类似。

使用软件：Stata。

# 合理预测模型

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429164529866.png)

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429164546728.png)

# 多元线性回归

跟拟合类似，对横截面数据进行回归。

一元线性回归，一般称 $y_i=\beta_0+\beta_1x_i+\mu_i$ 添加残差。

自变量和因变量可通过变量转化变成线性模型，$lny_i=\beta_0+\beta_1\ln{x_i}+\mu_i$, $y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\mu_i$

使用线性回归之前，需要对数据进行预处理。

## 回归系数

$y_i=\beta_0+\beta_1x_i+\mu_i$， 称 $\beta_0, \beta_1$ 为回归系数。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210422162439941.png)

## 虚拟变量

自变量中有定性变量，例如性别、地域，在回归中 $y_i = \beta_0 + \beta_1x_{1i} + ... +\delta_0d_0+...+\mu_i$

变量 $d_i$  取 0 或 1

避免完全 **多重共线性** 的影响，引入虚拟变量的个数一般是分类数减1。

## 假定满足

回归模型满足 4 个假定：

假定1：线性假定，因变量与自变量存在线性关系

假定2：严格外生性，所有的自变量跟扰动项不相关，保证估计出来的回归系数无偏且一致。

假定3：完全多重无关性，保证能估计出来，自变量满足列满秩，$X'X$ 可逆。

假定4：球型干扰项，同方差，无自相关，$E(\xi_i\xi_j|x) = 0$， $\xi$  为扰动项。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423164647820.png)

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423165346306.png)

### 内生性

就是由尚未添加的变量，该变量既跟 y 有关，也跟已添加的自变量有关。

![image-20210422162618842](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210422162618842.png)

即 $u_i$ 其实包含其他相关变量影响，导致估计不准确。

再以一个例子为例：

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210422162842676.png)

```matlab
%% 蒙特卡洛模拟：内生性会造成回归系数的巨大误差
times = 300; % 蒙特卡洛的次数
R = zeros(times, 1);  % 用来储存扰动项 u和x1 的相关系数
K = zeros(times, 1);  % 用来储存遗漏了x2之后，只用y对x1回归得到的回归系数 k
for i = 1: times
    n = 30;  % 样本数据量为n
    x1 = -10 + rand(n, 1) * 20;   % x1在-10和10上均匀分布，大小为30*1
    u1 = normrnd(0, 5, n, 1) - rand(n, 1);  % 随机生成一组符合正态分布的随机数
    x2 = 0.3 * x1 + u1;   % x2与x1的相关性不确定，随机生成x2
    u = normrnd(0, 1, n, 1);  % 扰动项u服从标准正态分布
    y = 0.5 + 2 * x1 + 5 * x2 + u;  % 构造y
    % 最小二乘法估计出来的k y=kx+b
    k = (n * sum(x1 .* y)-sum(x1) * sum(y))/(n * sum(x1 .* x1)-sum(x1)* sum(x1)); 
    K(i) = k;
    u = 5 * x2 + u;  %% 因为我们回归中忽略了 5*x2，所以扰动项要加上5*x2 （实际扰动存在内生性）
    r = corrcoef(x1, u);  % 2*2的相关系数矩阵 0说明不相关
    R(i) = r(2, 1);
end
plot(R, K, '*')
xlabel("x_1和u'的相关系数")
ylabel("k的估计值")
```

无内生性要求所有 **解释变量（X）** 跟 **扰动项** 不相关，确保其全部外生比较困难，但可以弱化此条件。

将解释变量分为核心解释变量和控制变量两类：

- **核心解释变量**：我们最感兴趣的变量，因此我们特别希望得到对其系数的一致估计（当样本容量无限增大时，收敛于待估计参数的真值）
- **控制变量**：我们可能对于这些变量本身并无太大兴趣；而之所以把它们也放入回归方程，主要是为了“控制住”那些对被解释变量有影响的遗漏因素。

在实际应用中，我们只要保证核心解释变量与𝝁不相关即可 （要求确保相关系数为0）。

### 多重共线性

某一解释变量可以由其他解释变量解释。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423141516797.png)

#### 假设检验

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423141902121.png)

#### 解决方法

如果发现存在多重共线性，可以采取以下处理方法：

1. 如果不关心具体的 **回归系数**，而只关心 **整个方程** 预测被解释变量的能力，则通常可以不必理会多重共线性（多重共线性的主要后果是使得对单个变量的贡献估计不准，但所有变量的整体效应仍可以较准确地估计）。
2. 关心具体的回归系数时，如果多重共线性并不影响所关心变量的 **显著性**，那么也可以不必理会；否则，需要增大样本容量，剔除导致严重共线性的变量（可能会有内生性的影响），或修改模型。

### 扰动项

在之前的回归分析中，我们都默认了扰动项 $\mu_i$ 是球型扰动项。

球型扰动项：满足 **同方差** 和 **无自相关** 两个条件。

横截面数据容易出现 *异方差* 的问题，时间序列数据容易出现 *自相关* 的问题。

#### 异方差

数据分布极度不平衡，如图示。$OLS$ 普通最小二乘法。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423140237418.png)

##### 假设检验

BP 检验或者怀特检验，原假设，扰动项不存在异方差， P 值小于0.05，如果在95%的置信水平下拒绝原假设，即我们认为扰动项存在异方差。

使用 Stata:

```stata
// 回归结束后使用
estat hettest, rhs iid
estat imtest, white
```

##### 解决方法

1. 使用OLS + 稳健的标准误，如果发现存在异方差，一种处理方法是，仍然进行OLS 回归，但使用稳健标准误。这是最简单，也是目前通用的方法。只要样本容量较大，即使在异方差的情况下，若使用稳健标准误，则所有参数估计、假设检验均可照常进行。
2. 广义最小二乘法GLS，缺点：只能用样本数据来估计，这样得到的结果不稳健，存在偶然性。

```stata
regress y x1 x2 ... xk, robust
```

## 逐步回归

**向前逐步回归**：将自变量逐个引入模型，每引入一个自变量后都要进行检验，显著时才加入回归模型。（缺点：随着以后其他自变量的引入，原来显著的自变量也可能又变为不显著了，但是，并没有将其及时从回归方程中剔除掉。）

**向后逐步回归**：先将所有变量均放入模型，之后尝试将其中一个自变量从模型中剔除，看整个模型解释因变量的变异是否有显著变化，之后将最没有解释力的那个自变量剔除；此过程不断迭代，直到没有自变量符合剔除的条件。（缺点：计算量大）

```stata
// stat 使用前需要剔除多重共线性
stepwise regress y x1 x2 ... xk, pe(0.05)
stepwise regress y x1 x2 ... xk, pr(0.05)
```

## 回归模型解释

思考模型 $y_i=\beta_0+\beta_1\ln{x_i}$ 怎么解释？

伍德里奇的《计量经济学导论，现代观点》有详细的论述：

取对数意味着原被解释变量对解释变量的 **弹性**，即百分比的变化而不是数值的变化。取对数有一些经验法则：

取对数的好处：（1）减弱数据的异方差性（2）如果变量本身不符合正态分布，取了对数后可能渐近服从正态分布（3）模型形式的需要，让模型具有经济学意义。

1. 与市场价值相关的，例如，价格、销售额、工资等都可以取对数
2. 以年度量的变量，如受教育年限、工作经历等通常不取对数
3. 比例变量，如失业率、参与率等，两者均可；
4. 变量取值必须是非负数，如果包含0，则可以对 y 取对数 ln(1+y)

取对数的好处：

1.  减弱数据的 **异方差性**
2.  如果变量本身不符合正态分布，取了对数后可能渐近服从正态分布
3.  模型形式的需要，让模型具有经济学意义

四类模型回归系数的解释：

1. 一元线性回归：𝑦 = 𝑎 + 𝑏𝑥+ 𝜇，x 每增加 1 个单位，y 平均变化 b 个单位
2. 双对数模型：𝑙𝑛𝑦= 𝑎+ 𝑏𝑙𝑛𝑥+ 𝜇，x 每增加 1%，y 平均变化b%
3. 半对数模型：𝑦 = 𝑎 + 𝑏𝑙𝑛𝑥+ 𝜇，x 每增加 1%，y 平均变化 b/100 个单位
4. 半对数模型：𝑙𝑛𝑦 = 𝑎+  𝑏𝑥+ 𝜇，x 每增加 1 个单位，y 平均变化 (100b)%

## 回归评价

预测型回归一般才会更看重 $R^2$。

解释型回归更多的关注模型整体 **显著性** 即可。

可以对模型进行调整，例如对数据取对数或者平方后再进行回归.

数据的分布极度不均匀，可能会导致 $R^2$ 较低。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423145809163.png)

在对 **显著** 的回归系数评价时，更为精准的研究影响评价量的重要因素（去除量纲的影响），我们可考虑使用 **标准化** 回归系数，标准化系数的绝对值越大，说明对因变量的影响就越大。

要先说明在 *F 检验* 下回归效应显著，然后讲解各个回归系数。

**某些错误的分析做法：**

不可先对变量进行归一化，否则不好解释或有不好的影响。

不要随便对模型变量（包含虚拟变量）复杂组合，否则不好解释。

##  Stata 使用

```stata
import excel "课堂中讲解的奶粉数据.xlsx", sheet("Sheet1") firstrow
// 定量变量的描述性统计
summarize 团购价元 评价量 商品毛重kg
// 定性变量的频数分布，并得到相应字母开头的虚拟变量
tabulate 配方 ,gen(A)
tabulate 奶源产地 ,gen(B)
tabulate 国产或进口 ,gen(C)
tabulate 适用年龄岁 ,gen(D)
tabulate 包装单位 ,gen(E)
tabulate 分类 ,gen(F)
tabulate 段位 ,gen(G)
// 下面进行回归
regress 评价量 团购价元 商品毛重kg
// 得到标准化回归系数
regress 评价量 团购价元 商品毛重kg, b
// 画出残差图
regress 评价量 团购价元 商品毛重kg A1 A2 A3 B1 B2 B3 B4 B5 B6 B7 B8 B9 C1 C2 D1 D2 D3 D4 D5 E1 E2 E3 E4 F1 F2 G1 G2 G3 G4
// 残差与拟合值的散点图
rvfplot 
// 残差与自变量团购价的散点图
rvpplot  团购价元

// 为什么评价量的拟合值会出现负数？
// 描述性统计并给出分位数对应的数值
summarize 评价量, d
// 作评价量的概率密度估计图
kdensity 评价量

// 异方差BP检验
estat hettest ,rhs iid
// 异方差怀特检验
estat imtest,white
// 使用OLS + 稳健的标准误
regress 评价量 团购价元 商品毛重kg A1 A2 A3 B1 B2 B3 B4 B5 B6 B7 B8 B9 C1 C2 D1 D2 D3 D4 D5 E1 E2 E3 E4 F1 F2 G1 G2 G3 G4, r

// 计算VIF
estat  vif
```

# 岭回归 / lasso 回归

使用软件：Stata。

主要是为了解决 **多重共线性** 的问题。

岭回归和 lasso 回归在 OLS 回归模型的损失函数上加上了不同的惩罚项，该惩罚项由 **回归系数的函数** 构成：

- 一方面，加入的惩罚项能够识别出模型中不重要的变量，对模型起到简化作用，可以看作逐步回归法的升级版

- 另一方面，加入的惩罚项能够让模型变得 **可估计**

## 岭回归

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423165603084.png)

所以选择不同 $\lambda$ 会得出不同的回归系数，选择 $\lambda$ 的方法：

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423165909057.png)

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423165933033.png)

最小化均方预测误差方法，取不同的 $\lambda$ 然后多次进行 K 折交叉验证，使得整个样本的 MSPE （均方预测误差）最小。

## lasso 回归

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210423170638310.png)

```stata
// 注意：这里自变量的量纲相同所以不用标准化，如果需要标准化，那么可以借助Matlab的zscore函数，或者直接使用SPSS（分析-描述统计-描述：在描述列表的方框左下角，看到“将标准化得分另存为变量（Z）之后点击打勾，然后确定。）
// Stata一次只能标准化一个变量，例如： egen Y = std(单产)  这个代码就表示将单产标准化，得到的变量记为Y
cvlasso 单产 种子费 化肥费 农药费 机械费 灌溉费, lopt seed(520) // seed 不同种子
```

stata 会给出各 $\lambda$ 的计算结果，以及提出 0 系数后的回归结果。

## 何时使用

最一般的OLS对数据进行回归，然后计算方差膨胀因子VIF，如果 VIF >10 则说明存在多重共线性的问题，此时我们需要对变量进行筛选。

# BP 神经算法

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429162305389.png)

BP 反向传播，能够根据误差再去调整差值。

神经网络的参数跟层数还有各样本输入特征值的数量密切相关，而且可以用来训练样本少而特征值多的数据集。

效果不错，相当万金油，但是具体解释麻烦，**简单数据集** 最好还是用其他模型。

使用软件：Matlab。

## Matlab 使用

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429163406220.png)

### 训练

设置数据集比例：

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429163156212.png)

训练算法有以下 3 种：

- Levenberg–Marquardt algorithm：能提供数非线性最小化（局部最小）的数值解，速度最快

- Scaled Conjugate Gradient
- Bayesian‐regularization：速度最慢，收敛次数多，拟合效果最好

一般我们还可以主动把数据集分割开，用这 3 种训练算法算出后再看效果。

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429163752681.png)

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429163818936.png)

![](https://hauk-blog.oss-cn-hangzhou.aliyuncs.com/blogimage-20210429163935237.png)

### 预测

```matlab
load data_Octane.mat
nftool
% 根据训练出来的网络 net 预测
sim(net, new_X(1,:)')

predict_y = zeros(10,1);
for i = 1: 10
    result = sim(net, new_X(i,:)');
    predict_y(i) = result;
end
disp('预测值为：')
disp(predict_y)
```

# 逻辑回归

又称 Logistic 回归，由于对已知数据分类，详见分类模型。

